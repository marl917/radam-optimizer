{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "# import standard PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter # TensorBoard support\n",
    "\n",
    "# import torchvision module to handle image manipulation\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# calculate train time, writing train data to files etc.\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)    \n",
    "\n",
    "import pytorch_warmup as warmup\n",
    "\n",
    "\n",
    "from collections import namedtuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAdam(Optimizer):   #available on this github repository : https://github.com/LiyuanLucasLiu/RAdam\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        \n",
    "        self.degenerated_to_sgd = degenerated_to_sgd\n",
    "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
    "            for param in params:\n",
    "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
    "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = group['buffer'][int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    elif self.degenerated_to_sgd:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = -1\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "                elif step_size > 0:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import train set Fashion Mnist\n",
    "train_set = torchvision.datasets.FashionMNIST(    \n",
    "    root = './data/FashionMNIST',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()                                 \n",
    "    ])\n",
    ")\n",
    "#import test set Fashion Mnist\n",
    "test_set = torchvision.datasets.FashionMNIST(root = \"./data/FashionMNIST\", train = False, download = True, transform = transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module): # Build the neural network, expand on top of nn.Module\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # define layers\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "    self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "    self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "    self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "  # define forward function\n",
    "  def forward(self, t):\n",
    "    # conv 1\n",
    "    t = self.conv1(t)\n",
    "    t = F.relu(t)\n",
    "    t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "    # conv 2\n",
    "    t = self.conv2(t)\n",
    "    t = F.relu(t)\n",
    "    t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "    # fc1\n",
    "    t = t.reshape(-1, 12*4*4)\n",
    "    t = self.fc1(t)\n",
    "    t = F.relu(t)\n",
    "\n",
    "    # fc2\n",
    "    t = self.fc2(t)\n",
    "    t = F.relu(t)\n",
    "\n",
    "    # output\n",
    "    t = self.out(t)\n",
    "    # don't need softmax here since we'll use cross-entropy as activation.\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class, help track loss, accuracy, epoch time, run time, \n",
    "# hyper-parameters etc. Also record to TensorBoard and write into csv, json\n",
    "class RunManager():\n",
    "  def __init__(self):\n",
    "\n",
    "    # tracking every epoch count, loss, accuracy, time\n",
    "    self.epoch_count = 0\n",
    "    self.epoch_loss = 0\n",
    "    self.epoch_num_correct = 0\n",
    "    self.epoch_start_time = None\n",
    "\n",
    "    # tracking every run count, run data, hyper-params used, time\n",
    "    self.run_params = None\n",
    "    self.run_count = 0\n",
    "    self.run_data = []\n",
    "    self.run_start_time = None\n",
    "\n",
    "    # record model, loader and TensorBoard \n",
    "    self.network = None\n",
    "    self.trainloader = None\n",
    "    self.testloader=None\n",
    "    self.tb = None\n",
    "\n",
    "  # record the count, hyper-param, model, loader of each run\n",
    "  # record sample images and network graph to TensorBoard  \n",
    "  def begin_run(self, run, network, trainloader, testloader):\n",
    "\n",
    "    self.run_start_time = time.time()\n",
    "\n",
    "    self.run_params = run\n",
    "    self.run_count += 1\n",
    "\n",
    "    self.network = network\n",
    "    self.trainloader = trainloader\n",
    "    self.testloader=testloader\n",
    "    self.tb = SummaryWriter(comment='num:{}'.format(run))\n",
    "\n",
    "    images, labels = next(iter(self.trainloader))\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "    self.tb.add_image('images', grid)\n",
    "    self.tb.add_graph(self.network, images)\n",
    "\n",
    "  # when run ends, close TensorBoard, zero epoch count\n",
    "  def end_run(self):\n",
    "    self.tb.close()\n",
    "    self.epoch_count = 0\n",
    "\n",
    "  # zero epoch count, loss, accuracy, \n",
    "  def begin_epoch(self):\n",
    "    self.epoch_start_time = time.time()\n",
    "\n",
    "    self.epoch_count += 1\n",
    "    self.epoch_loss = 0\n",
    "    self.epoch_num_correct = 0\n",
    "\n",
    "  # \n",
    "  def end_epoch(self):\n",
    "    # calculate epoch duration and run duration(accumulate)\n",
    "    epoch_duration = time.time() - self.epoch_start_time\n",
    "    run_duration = time.time() - self.run_start_time\n",
    "\n",
    "    # record epoch loss and accuracy\n",
    "    loss = self.epoch_loss / len(self.trainloader.dataset)\n",
    "    accuracy = self.epoch_num_correct / len(self.testloader.dataset)\n",
    "\n",
    "    # Record epoch loss and accuracy to TensorBoard \n",
    "    self.tb.add_scalar('Loss', loss, self.epoch_count)\n",
    "    self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)\n",
    "\n",
    "    # Record params to TensorBoard\n",
    "    for name, param in self.network.named_parameters():\n",
    "      self.tb.add_histogram(name, param, self.epoch_count)\n",
    "      self.tb.add_histogram('grad', param.grad, self.epoch_count)\n",
    "    \n",
    "    # Write into 'results' (OrderedDict) for all run related data\n",
    "    results = OrderedDict()\n",
    "    results[\"run\"] = self.run_count\n",
    "    results[\"epoch\"] = self.epoch_count\n",
    "    results[\"loss\"] = loss\n",
    "    results[\"accuracy\"] = accuracy\n",
    "    results[\"epoch duration\"] = epoch_duration\n",
    "    results[\"run duration\"] = run_duration\n",
    "\n",
    "    # Record hyper-params into 'results'\n",
    "    for k,v in self.run_params._asdict().items(): results[k] = v\n",
    "    self.run_data.append(results)\n",
    "    df = pd.DataFrame.from_dict(self.run_data, orient = 'columns')\n",
    "\n",
    "    # display epoch information and show progress\n",
    "    clear_output(wait=True)\n",
    "    display(df)\n",
    "\n",
    "  # accumulate loss of batch into entire epoch loss\n",
    "  def track_loss(self, loss):\n",
    "    # multiply batch size so variety of batch sizes can be compared\n",
    "    self.epoch_loss += loss.item() * self.trainloader.batch_size\n",
    "\n",
    "  # accumulate number of corrects of batch into entire epoch num_correct\n",
    "  def track_num_correct(self, preds, labels):\n",
    "    self.epoch_num_correct += self._get_num_correct(preds, labels)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def _get_num_correct(self, preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "  \n",
    "  # save end results of all runs into csv, json for further analysis\n",
    "  def save(self, fileName):\n",
    "\n",
    "    pd.DataFrame.from_dict(\n",
    "        self.run_data, \n",
    "        orient = 'columns',\n",
    "    ).to_csv('{}.csv'.format(fileName))\n",
    "\n",
    "    with open('{}.json'.format(fileName), 'w', encoding='utf-8') as f:\n",
    "      json.dump(self.run_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.296278</td>\n",
       "      <td>0.2926</td>\n",
       "      <td>7.972315</td>\n",
       "      <td>8.414249</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.534780</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>7.699022</td>\n",
       "      <td>16.221006</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.781895</td>\n",
       "      <td>0.7226</td>\n",
       "      <td>7.600344</td>\n",
       "      <td>23.927817</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.664426</td>\n",
       "      <td>0.7554</td>\n",
       "      <td>8.437973</td>\n",
       "      <td>32.451986</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.591111</td>\n",
       "      <td>0.7831</td>\n",
       "      <td>7.687317</td>\n",
       "      <td>40.284343</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.537486</td>\n",
       "      <td>0.8042</td>\n",
       "      <td>7.531555</td>\n",
       "      <td>47.921618</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.494863</td>\n",
       "      <td>0.8204</td>\n",
       "      <td>7.656742</td>\n",
       "      <td>55.680879</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.458215</td>\n",
       "      <td>0.8291</td>\n",
       "      <td>7.682509</td>\n",
       "      <td>63.471893</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.427281</td>\n",
       "      <td>0.8436</td>\n",
       "      <td>8.053107</td>\n",
       "      <td>71.632058</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.401865</td>\n",
       "      <td>0.8544</td>\n",
       "      <td>7.894451</td>\n",
       "      <td>79.632238</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.381026</td>\n",
       "      <td>0.8603</td>\n",
       "      <td>7.692039</td>\n",
       "      <td>87.442335</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.362603</td>\n",
       "      <td>0.8651</td>\n",
       "      <td>7.652665</td>\n",
       "      <td>95.203749</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.348109</td>\n",
       "      <td>0.8691</td>\n",
       "      <td>7.649334</td>\n",
       "      <td>102.975017</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.336141</td>\n",
       "      <td>0.8703</td>\n",
       "      <td>7.924811</td>\n",
       "      <td>111.044702</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.326608</td>\n",
       "      <td>0.8741</td>\n",
       "      <td>7.802947</td>\n",
       "      <td>118.957869</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.318292</td>\n",
       "      <td>0.8762</td>\n",
       "      <td>7.568999</td>\n",
       "      <td>126.626772</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.308437</td>\n",
       "      <td>0.8757</td>\n",
       "      <td>7.972955</td>\n",
       "      <td>134.705564</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.300257</td>\n",
       "      <td>0.8778</td>\n",
       "      <td>8.026242</td>\n",
       "      <td>142.850697</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.293795</td>\n",
       "      <td>0.8797</td>\n",
       "      <td>7.701319</td>\n",
       "      <td>150.670157</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.286690</td>\n",
       "      <td>0.8798</td>\n",
       "      <td>7.717503</td>\n",
       "      <td>158.498762</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.279521</td>\n",
       "      <td>0.8806</td>\n",
       "      <td>7.697819</td>\n",
       "      <td>166.324673</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.272133</td>\n",
       "      <td>0.8822</td>\n",
       "      <td>7.781755</td>\n",
       "      <td>174.230624</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>7.927362</td>\n",
       "      <td>182.270140</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.260535</td>\n",
       "      <td>0.8812</td>\n",
       "      <td>8.270430</td>\n",
       "      <td>190.687695</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.255336</td>\n",
       "      <td>0.8855</td>\n",
       "      <td>7.847279</td>\n",
       "      <td>198.655592</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.253235</td>\n",
       "      <td>0.8879</td>\n",
       "      <td>8.007376</td>\n",
       "      <td>206.806888</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.253195</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>7.749475</td>\n",
       "      <td>214.679284</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.251933</td>\n",
       "      <td>0.8868</td>\n",
       "      <td>7.717396</td>\n",
       "      <td>222.540974</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.248773</td>\n",
       "      <td>0.8910</td>\n",
       "      <td>7.596611</td>\n",
       "      <td>230.257953</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.240458</td>\n",
       "      <td>0.8933</td>\n",
       "      <td>7.820448</td>\n",
       "      <td>238.205063</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.233506</td>\n",
       "      <td>0.8919</td>\n",
       "      <td>8.069831</td>\n",
       "      <td>246.397265</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.227761</td>\n",
       "      <td>0.8934</td>\n",
       "      <td>8.266217</td>\n",
       "      <td>254.760758</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.222232</td>\n",
       "      <td>0.8953</td>\n",
       "      <td>8.163303</td>\n",
       "      <td>263.025638</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0.217851</td>\n",
       "      <td>0.8964</td>\n",
       "      <td>7.779191</td>\n",
       "      <td>270.927552</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>0.216133</td>\n",
       "      <td>0.8939</td>\n",
       "      <td>7.713536</td>\n",
       "      <td>278.779668</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0.214065</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>7.621335</td>\n",
       "      <td>286.543993</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0.208256</td>\n",
       "      <td>0.8930</td>\n",
       "      <td>8.029845</td>\n",
       "      <td>294.700901</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0.203155</td>\n",
       "      <td>0.8964</td>\n",
       "      <td>7.717090</td>\n",
       "      <td>302.482292</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.198844</td>\n",
       "      <td>0.8967</td>\n",
       "      <td>7.892488</td>\n",
       "      <td>310.502299</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0.194937</td>\n",
       "      <td>0.8983</td>\n",
       "      <td>7.677106</td>\n",
       "      <td>318.308910</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.191691</td>\n",
       "      <td>0.8985</td>\n",
       "      <td>7.453853</td>\n",
       "      <td>325.866808</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0.189873</td>\n",
       "      <td>0.8993</td>\n",
       "      <td>7.427520</td>\n",
       "      <td>333.383156</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>0.186947</td>\n",
       "      <td>0.8997</td>\n",
       "      <td>7.881592</td>\n",
       "      <td>341.385206</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.184923</td>\n",
       "      <td>0.9006</td>\n",
       "      <td>8.175340</td>\n",
       "      <td>349.693291</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.183121</td>\n",
       "      <td>0.9005</td>\n",
       "      <td>7.801708</td>\n",
       "      <td>357.646162</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0.181728</td>\n",
       "      <td>0.9007</td>\n",
       "      <td>7.498239</td>\n",
       "      <td>365.272024</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0.180272</td>\n",
       "      <td>0.9007</td>\n",
       "      <td>7.749233</td>\n",
       "      <td>373.146333</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>0.178790</td>\n",
       "      <td>0.9009</td>\n",
       "      <td>7.817646</td>\n",
       "      <td>381.089757</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>0.177862</td>\n",
       "      <td>0.9008</td>\n",
       "      <td>7.882346</td>\n",
       "      <td>389.115481</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.177283</td>\n",
       "      <td>0.9006</td>\n",
       "      <td>7.993279</td>\n",
       "      <td>397.233993</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    run  epoch      loss  accuracy  epoch duration  run duration    lr  \\\n",
       "0     1      1  2.296278    0.2926        7.972315      8.414249  0.01   \n",
       "1     1      2  1.534780    0.6610        7.699022     16.221006  0.01   \n",
       "2     1      3  0.781895    0.7226        7.600344     23.927817  0.01   \n",
       "3     1      4  0.664426    0.7554        8.437973     32.451986  0.01   \n",
       "4     1      5  0.591111    0.7831        7.687317     40.284343  0.01   \n",
       "5     1      6  0.537486    0.8042        7.531555     47.921618  0.01   \n",
       "6     1      7  0.494863    0.8204        7.656742     55.680879  0.01   \n",
       "7     1      8  0.458215    0.8291        7.682509     63.471893  0.01   \n",
       "8     1      9  0.427281    0.8436        8.053107     71.632058  0.01   \n",
       "9     1     10  0.401865    0.8544        7.894451     79.632238  0.01   \n",
       "10    1     11  0.381026    0.8603        7.692039     87.442335  0.01   \n",
       "11    1     12  0.362603    0.8651        7.652665     95.203749  0.01   \n",
       "12    1     13  0.348109    0.8691        7.649334    102.975017  0.01   \n",
       "13    1     14  0.336141    0.8703        7.924811    111.044702  0.01   \n",
       "14    1     15  0.326608    0.8741        7.802947    118.957869  0.01   \n",
       "15    1     16  0.318292    0.8762        7.568999    126.626772  0.01   \n",
       "16    1     17  0.308437    0.8757        7.972955    134.705564  0.01   \n",
       "17    1     18  0.300257    0.8778        8.026242    142.850697  0.01   \n",
       "18    1     19  0.293795    0.8797        7.701319    150.670157  0.01   \n",
       "19    1     20  0.286690    0.8798        7.717503    158.498762  0.01   \n",
       "20    1     21  0.279521    0.8806        7.697819    166.324673  0.01   \n",
       "21    1     22  0.272133    0.8822        7.781755    174.230624  0.01   \n",
       "22    1     23  0.266075    0.8805        7.927362    182.270140  0.01   \n",
       "23    1     24  0.260535    0.8812        8.270430    190.687695  0.01   \n",
       "24    1     25  0.255336    0.8855        7.847279    198.655592  0.01   \n",
       "25    1     26  0.253235    0.8879        8.007376    206.806888  0.01   \n",
       "26    1     27  0.253195    0.8781        7.749475    214.679284  0.01   \n",
       "27    1     28  0.251933    0.8868        7.717396    222.540974  0.01   \n",
       "28    1     29  0.248773    0.8910        7.596611    230.257953  0.01   \n",
       "29    1     30  0.240458    0.8933        7.820448    238.205063  0.01   \n",
       "30    1     31  0.233506    0.8919        8.069831    246.397265  0.01   \n",
       "31    1     32  0.227761    0.8934        8.266217    254.760758  0.01   \n",
       "32    1     33  0.222232    0.8953        8.163303    263.025638  0.01   \n",
       "33    1     34  0.217851    0.8964        7.779191    270.927552  0.01   \n",
       "34    1     35  0.216133    0.8939        7.713536    278.779668  0.01   \n",
       "35    1     36  0.214065    0.8917        7.621335    286.543993  0.01   \n",
       "36    1     37  0.208256    0.8930        8.029845    294.700901  0.01   \n",
       "37    1     38  0.203155    0.8964        7.717090    302.482292  0.01   \n",
       "38    1     39  0.198844    0.8967        7.892488    310.502299  0.01   \n",
       "39    1     40  0.194937    0.8983        7.677106    318.308910  0.01   \n",
       "40    1     41  0.191691    0.8985        7.453853    325.866808  0.01   \n",
       "41    1     42  0.189873    0.8993        7.427520    333.383156  0.01   \n",
       "42    1     43  0.186947    0.8997        7.881592    341.385206  0.01   \n",
       "43    1     44  0.184923    0.9006        8.175340    349.693291  0.01   \n",
       "44    1     45  0.183121    0.9005        7.801708    357.646162  0.01   \n",
       "45    1     46  0.181728    0.9007        7.498239    365.272024  0.01   \n",
       "46    1     47  0.180272    0.9007        7.749233    373.146333  0.01   \n",
       "47    1     48  0.178790    0.9009        7.817646    381.089757  0.01   \n",
       "48    1     49  0.177862    0.9008        7.882346    389.115481  0.01   \n",
       "49    1     50  0.177283    0.9006        7.993279    397.233993  0.01   \n",
       "\n",
       "    batch_size optimizer  epochs  \n",
       "0         1000     AdamW      50  \n",
       "1         1000     AdamW      50  \n",
       "2         1000     AdamW      50  \n",
       "3         1000     AdamW      50  \n",
       "4         1000     AdamW      50  \n",
       "5         1000     AdamW      50  \n",
       "6         1000     AdamW      50  \n",
       "7         1000     AdamW      50  \n",
       "8         1000     AdamW      50  \n",
       "9         1000     AdamW      50  \n",
       "10        1000     AdamW      50  \n",
       "11        1000     AdamW      50  \n",
       "12        1000     AdamW      50  \n",
       "13        1000     AdamW      50  \n",
       "14        1000     AdamW      50  \n",
       "15        1000     AdamW      50  \n",
       "16        1000     AdamW      50  \n",
       "17        1000     AdamW      50  \n",
       "18        1000     AdamW      50  \n",
       "19        1000     AdamW      50  \n",
       "20        1000     AdamW      50  \n",
       "21        1000     AdamW      50  \n",
       "22        1000     AdamW      50  \n",
       "23        1000     AdamW      50  \n",
       "24        1000     AdamW      50  \n",
       "25        1000     AdamW      50  \n",
       "26        1000     AdamW      50  \n",
       "27        1000     AdamW      50  \n",
       "28        1000     AdamW      50  \n",
       "29        1000     AdamW      50  \n",
       "30        1000     AdamW      50  \n",
       "31        1000     AdamW      50  \n",
       "32        1000     AdamW      50  \n",
       "33        1000     AdamW      50  \n",
       "34        1000     AdamW      50  \n",
       "35        1000     AdamW      50  \n",
       "36        1000     AdamW      50  \n",
       "37        1000     AdamW      50  \n",
       "38        1000     AdamW      50  \n",
       "39        1000     AdamW      50  \n",
       "40        1000     AdamW      50  \n",
       "41        1000     AdamW      50  \n",
       "42        1000     AdamW      50  \n",
       "43        1000     AdamW      50  \n",
       "44        1000     AdamW      50  \n",
       "45        1000     AdamW      50  \n",
       "46        1000     AdamW      50  \n",
       "47        1000     AdamW      50  \n",
       "48        1000     AdamW      50  \n",
       "49        1000     AdamW      50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Run = namedtuple('Run', 'lr batch_size optimizer epochs') #define parameters used to train the model (lr=learning rate, optimizer=Adam or Radam)\n",
    "\n",
    "\n",
    "m = RunManager()\n",
    "network = Network()\n",
    "\n",
    "def train(run,m,network):\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size = run.batch_size)\n",
    "    testloader= torch.utils.data.DataLoader(test_set, batch_size = run.batch_size)\n",
    "    epochs=run.epochs\n",
    "    m.begin_run(run, network, trainloader, testloader)\n",
    "    \n",
    "    if run.optimizer==\"Adam\":                                                            #choose the right optimizer according to the input run.optimizer\n",
    "        optimizer = optim.Adam(network.parameters(), lr=run.lr,betas=(0.9, 0.999))\n",
    "        \n",
    "    elif run.optimizer==\"AdamW\":\n",
    "        optimizer = torch.optim.AdamW(network.parameters(), lr=run.lr, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "        num_steps = len(trainloader) * epochs\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\n",
    "        warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n",
    "        \n",
    "    elif run.optimizer==\"RAdam\":\n",
    "        optimizer = RAdam(network.parameters(), lr=run.lr, betas=(0.9, 0.99), weight_decay=0)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        m.begin_epoch()\n",
    "        for batch in trainloader:     \n",
    "            images = batch[0]\n",
    "            labels = batch[1]\n",
    "            preds = network(images)\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            if run.optimizer==\"AdamW\":    #warmup step\n",
    "                lr_scheduler.step()\n",
    "                warmup_scheduler.dampen()\n",
    "            \n",
    "            m.track_loss(loss)   #update training loss\n",
    "\n",
    "        for batch in testloader:\n",
    "            images = batch[0]\n",
    "            labels = batch[1]\n",
    "            preds = network(images)\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "\n",
    "            m.track_num_correct(preds, labels) #update test accuracy\n",
    "\n",
    "\n",
    "        m.end_epoch()\n",
    "    m.end_run()\n",
    "    m.save('results')\n",
    "#AdamW stands for Adam with warmup while Radam is rectified Adam\n",
    "\n",
    "first=Run(lr=0.01, batch_size=1000, optimizer=\"AdamW\", epochs=50)\n",
    "train(first,m,network)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_envbis",
   "language": "python",
   "name": "pytorch_envbis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
